{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train DeepMind GNN Physics Simulator\n",
        "This notebook automatically provisions a Kaggle GPU Environment to run the DeepMind Graph Neural Network Physics Simulator optimization pipeline natively.\n",
        "It pulls the complete `WaterDrop` dataset via DeepMind's bucket and trains via `train.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "env_setup"
      },
      "outputs": [],
      "source": [
        "!rm -rf nans-projekat\n",
        "!git clone https://github.com/MarkoMile/gnn-physics-simulator.git\n",
        "%cd gnn-physics-simulator\n",
        "\n",
        "# Install PyTorch Geometric and its dependencies aligned for Kaggle's CUDA 12 environment\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.1.2+cu121.html\n",
        "!pip install torch-geometric\n",
        "\n",
        "# Install utility dependencies (Weights & Biases, PyYAML, TF Data Parser)\n",
        "!pip install wandb pyyaml tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqdm_patch"
      },
      "outputs": [],
      "source": [
        "import tqdm.notebook\n",
        "import tqdm.auto\n",
        "\n",
        "# Force the auto-detector to use the notebook progressive version to avoid console flooding\n",
        "tqdm.auto.tqdm = tqdm.notebook.tqdm\n",
        "tqdm.auto.trange = tqdm.notebook.trange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset_fetch"
      },
      "outputs": [],
      "source": [
        "# Fetch the full multi-GB DeepMind WaterDrop dataset straight from Google Cloud\n",
        "!python scripts/download_dataset.py WaterDrop\n",
        "\n",
        "import yaml\n",
        "\n",
        "# Patch the config.yaml dynamically to target the massive dataset instead of the tiny sample\n",
        "with open('config.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "    \n",
        "config['data']['dataset_path'] = 'data/WaterDrop'\n",
        "config['data']['dataset_format'] = 'tfrecord'\n",
        "\n",
        "with open('config.yaml', 'w') as f:\n",
        "    yaml.dump(config, f)\n",
        "    \n",
        "print(\"Patched config.yaml successfully to target full data/WaterDrop!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wandb_auth"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wandb\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "try:\n",
        "    user_secrets = UserSecretsClient()\n",
        "    wandb_api_key = user_secrets.get_secret(\"wandb_api_key\")\n",
        "    os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
        "    \n",
        "    wandb.login()\n",
        "    print(\"WandB logged in successfully using Kaggle Secrets.\")\n",
        "except Exception as e:\n",
        "    print(f\"WandB login skipped/failed: {e}\\nEnsure you have added 'wandb_api_key' to your Kaggle Secrets if you want telemetry.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_execute"
      },
      "outputs": [],
      "source": [
        "# Execute the global training loop with WandB telemetry enabled!\n",
        "!python train.py --config config.yaml --wandb"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
